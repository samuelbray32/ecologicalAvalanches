import numpy as np

def fitSet(data, m=2,tau=1,P=1):
    """
    Define fitting vectors and predictions from historical data
    data = historical data
    m = embedding dimension
    tau = lag time between embedded points
    P = forecasting time
    """
    xj=np.zeros((m+1,data.size-P-1-(m-1)*tau))
    xj[0,:]=1
    for i in range (1+m*tau,data.size-P):
        xj[1:,i-1-m*tau]=data[i-(m)*tau:i:tau]
    Sj=data[-xj.shape[1]:]
    return xj, Sj
    
def fitSet_ASE(data,m=2,tau=1, P=1, sub=1,zeroBuf=0):
    """
    Define fitting vectors and predictions from historical data
    data = list of ASE curves
    m = embedding dimension
    tau = lag time between embedded points
    P = forecasting time
    sub = subsampling of ASE timepoints when generating big extrapolated sets--speeds up search for nearest neighbors in prediction
    zeroBuf = zero pads the extrapolated fluctuations, unused in this paper
    """
    xFit=np.zeros((m+1,1))
    xFit[0,0]=1
    Pred=np.array([0])
    
    for j in range(len(data)):
        if zeroBuf>0:
            z=np.zeros(zeroBuf)
            data[j]=np.append(data[j],z)
        if data[j].size<m*tau+P+1:
            continue
        Nf=len(data[j])
        if Nf-P-1-(m-1)*tau<=0:
            continue
        xj=np.zeros((m+1,Nf-P-1-(m-1)*tau))
        xj[0,:]=1
#        print(xj.shape)
        for i in range (1+m*tau,data[j].size-P):
            if data[j][i:i+(m)*tau:tau].size<m:
#                print('ex')
                continue
            xj[1:,i-1-m*tau]=data[j][i-(m)*tau:i:tau]
        Sj=data[j][-xj.shape[1]:]
        subk=min(sub,int(.1*data[j].size)) #makes sure subsampling doesn't eliminate short events
        subk=max(subk,1)
        xFit=np.append(xFit,xj[:,::subk],1)
        Pred=np.append(Pred,Sj[::subk])
    return xFit[:,:], Pred

def predictability(xj,Sj,test,m=2,tau=1,P=1,k=1,p=1,subi=1,subj=1,tSet =[], skip=False):
    """
    sMap forecasting algorithm used to compare performance vs training data
    xj = train data input, generated by one of the fit set functions
    Sj = correct predictions for training set
    test = 1D vect of data want to predict
    m = embedding dimension
    tau = lag time between embedded points
    P = forecasting time
    k = number of nearest neighbors to use in local weighted regression
    p = scaling exponent for weight of neighbors with distance
    subi = subsampling of test data (if evaluating on big benchmarking sets)
    subj = subsampling of training data (if you want to speed up nearest neighbor search, compromise to accuracy)
    tSet = defines a subset of test data to exclude (if want to only evaluate at certain points)
    skip = whether to skip negatively valued points (doesn't come up in this paper)
    """
    
    #Prediction algorithm--fed fit set--userFriendly
    
    #define test vectors and correct predictions
    xi=np.zeros((m+1,test.size-P-1-(m-1)*tau))
    xi[0,:]=1
    for i in range (1+m*tau,test.size-P):
        xi[1:,i-1-m*tau]=test[i-(m)*tau:i:tau]
    Si=test[-xi.shape[1]:]
    #subsampling
    xi=xi[:,::subi]
    Si=Si[::subi]
    xj=xj[:,::subj]
    Sj=Sj[::subj]
    #predicted values
    V=[]
    #True values to return
    Sr=[]
    for i in range(xi.shape[1]):
        if skip and Si[i]<=0:
            continue
        if i in tSet:
            continue
        #calculate distances and sort for closest
        d=np.linalg.norm(xj.T-xi[:,i],ord=2,axis=1)
        ind=np.argpartition(d,k)[:k]
        bad=np.where(d==0)
        if len(d[np.where(d>0)])==0:
            d[bad]=1
        else:
            d[bad]=min(d[np.where(d>0)])/10
        #define lin regression and solve
        W=np.sqrt(np.diag(d[ind]**-p))
        b=np.dot(Sj[ind],W)
        A=np.dot(W,(xj[:,ind]).T)
        alpha=np.linalg.lstsq(A,b)[0]
        V.append(max(np.dot(alpha,xi[:,i]),0))
        Sr.append(Si[i])
    return np.array(V),np.array(Sr)


