#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Apr  5 11:00:05 2019

@author: gary
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Apr  2 12:26:19 2019

@author: gary
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 28 10:08:32 2019

@author: gary
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Feb 18 09:13:39 2019

@author: gary
"""


# In[]
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from scipy.integrate import odeint
import scipy.ndimage
import time
from sklearn.metrics import r2_score
import pickle
matplotlib.rc('pdf', fonttype=42)
matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['ps.fonttype'] = 42
matplotlib.rcParams['svg.fonttype'] = 'none'


def avFamily(normPath,Tfam,b,gam=1.5):
    #given system scaling and norm shape, returns family of avalanche curves
    yNorm=np.load(normPath)
    xNorm=np.linspace(0,1,yNorm.size)
    yFunc=scipy.interpolate.interp1d(xNorm,yNorm,kind='quadratic')
    def xT(T):
        return np.linspace(0,T,T)
    def yT(T):
        return b*T**(gam-1)*yFunc(np.linspace(0,1,T))
    t=[]
    S=[]
    for dur in Tfam:
        t.append(xT(dur))
        S.append(yT(dur))
    return t,S
   
def fitSet(dat,m,tau,Tp,sub=1,zeroBuf=0):
    #Function to turn family of curves into fitting data 
    #takes in list of dat curves returns fit and prediction vectors
    #defines fit vectors and correct predictions
    xFit=np.zeros((m+1,1))
    xFit[0,0]=1
    Pred=np.array([0])
    
    for j in range(len(dat)):
        if zeroBuf>0:
            z=np.zeros(zeroBuf)
            dat[j]=np.append(dat[j],z)
#            dat[j]=np.append(z,dat[j])        
        
#        print(dat[j].size)
        if dat[j].size<m*tau+Tp+1:
            continue
        Nf=len(dat[j])
        if Nf-Tp-1-(m-1)*tau<=0:
            continue
        xj=np.zeros((m+1,Nf-Tp-1-(m-1)*tau))
        xj[0,:]=1
#        print(xj.shape)
        for i in range (1+m*tau,dat[j].size-Tp):
            if dat[j][i:i+(m)*tau:tau].size<m:
#                print('ex')
                continue
            xj[1:,i-1-m*tau]=dat[j][i-(m)*tau:i:tau]
        Sj=dat[j][-xj.shape[1]:]
        subk=min(sub,int(.1*dat[j].size))
        subk=max(subk,1)
        xFit=np.append(xFit,xj[:,::subk],1)
        Pred=np.append(Pred,Sj[::subk])
    return xFit[:,:], Pred

def avalancheDist_mult(data,thresh,t=0):
    #extracts location and size of avalanches
    #use in fitting of b for data
    if t==0:
        t=np.linspace(0,data.size-1,data.size)
    st=[]
    en=[]
    dt=np.zeros(t.size)
    dt[1:]=t[1:]-t[:-1]
    for i in range (data.size):
        if len(st)==len(en):
            if data[i]>=thresh:
                st.append(i)
            continue
        if len(st)>len(en):
            if data[i]<thresh:
                en.append(i)
    if len(st)>len(en):
        en.append(data.size-1)
    st=np.array(st).astype(int)
    en=np.array(en).astype(int)
    #print(st.shape)
    T=t[en]-t[st]
    S=np.zeros(T.size)
    for i in range (st.size):
        S[i]=np.sum((data[st[i]:en[i]]-thresh)*dt[st[i]:en[i]])
    #Pathological case when small dataset
    S=np.abs(S)
    #Safety check for size one avalanches:
    ind=np.where(S>0)
    S=S[ind]
    T=T[ind]
    #Pathological case when small dataset
    if len(T)==0:
        pl=np.where(data>thresh)
        if np.sum(data[pl])>0:
            T=np.array([len(pl)])
            S=np.array([np.sum(data[pl])])
    return T,S

def bFit(T,S,gam=1.66):
    #Gets the log intercept value b needed to extrapolate family of curves
    from scipy.optimize import curve_fit
    def func(t,lb):
        return gam*t+lb
    T=np.log10(T)
    S=np.log10(S)
    popt,pcov=curve_fit(func, T, S)
    return 10**popt[0]


def predictability_Av(xj,Sj,test,m=2,tau=1,P=1,k=1,p=1,subi=1,subj=1,skip=False,tSet=[]):
    #Prediction algorithm--fed fit set--userFriendly
    
    #define test vectors and correct predictions
    xi=np.zeros((m+1,test.size-P-1-(m-1)*tau))
    xi[0,:]=1
    for i in range (1+m*tau,test.size-P):
        xi[1:,i-1-m*tau]=test[i-(m)*tau:i:tau]
    Si=test[-xi.shape[1]:]
    #subsampling
    xi=xi[:,::subi]
    Si=Si[::subi]
    xj=xj[:,::subj]
    Sj=Sj[::subj]
    #predicted values
    V=[]
    #True values to return
    Sr=[]
    for i in range(xi.shape[1]):
        if skip and Si[i]<=0:
            continue
        if i in tSet:
            continue
        #calculate distances and sort for closest
        #max norm of dif vector
#        d=np.max(np.abs(xj.T-xi[:,i]),1)
#        ind=d.argsort()[:k][::-1]
        #n-norm of dif vector
        d=np.linalg.norm(xj.T-xi[:,i],ord=2,axis=1)
#        ind=d.argsort()[:k][::-1]
        ind=np.argpartition(d,k)[:k]
        bad=np.where(d==0)
        if len(d[np.where(d>0)])==0:
            d[bad]=1
        else:
            d[bad]=min(d[np.where(d>0)])/10
        #define lin regression and solve
        W=np.sqrt(np.diag(d[ind]**-p))
        b=np.dot(Sj[ind],W)
        A=np.dot(W,(xj[:,ind]).T)
        alpha=np.linalg.lstsq(A,b)[0]
        #calculate error for this test
        #np.dot(alpha,xi[:,i])
        V.append(max(np.dot(alpha,xi[:,i]),0))
        Sr.append(Si[i])
#    if skip:
#        return np.array(V),Si[np.where(Si>0)]
    return np.array(V),np.array(Sr)

def predictability_Av2(xj,Sj,test,m=2,tau=1,P=1,k=1,p=1,subi=1,subj=1):
    #Prediction algorithm--fed fit set--userFriendly
    
    #define test vectors and correct predictions
    xi=np.zeros((m,test.size-P-1-(m-1)*tau))
#    xi[0,:]=1
    for i in range (1+m*tau,test.size-P):
        xi[:,i-1-m*tau]=test[i-(m)*tau:i:tau]
    Si=test[-xi.shape[1]:]
    #subsampling
    xi=xi[:,::subi]
    Si=Si[::subi]
    xj=xj[:,::subj]
    Sj=Sj[::subj]
    #predicted values
    V=[]
    for i in range(xi.shape[1]):
        #calculate distances and sort for closest
        #max norm of dif vector
#        d=np.max(np.abs(xj.T-xi[:,i]),1)
#        ind=d.argsort()[:k][::-1]
        #n-norm of dif vector
        d=np.linalg.norm(xj.T-xi[:,i],ord=2,axis=1)
#        ind=d.argsort()[:k][::-1]
        ind=np.argpartition(d,k)[:k]
        bad=np.where(d==0)
        if len(d[np.where(d>0)])==0:
            d[bad]=1
        else:
            d[bad]=min(d[np.where(d>0)])/10
        #define weighted average
        V.append(max(np.average(Sj[ind],weights=d[ind]**-p),0))
    return np.array(V),Si

def predictability_naive(fit,test,m=2,tau=1,P=1,k=1,p=1,subi=1,subj=1,skip=False,tSet=[]):
    #Prediction algorithm--fed fit data--userFriendly
    #Use for estimation with know assumption of dynamics
    #Define fitting vectors and predictions
    xj=np.zeros((m+1,fit.size-P-1-(m-1)*tau))
    xj[0,:]=1
    for i in range (1+m*tau,fit.size-P):
        xj[1:,i-1-m*tau]=fit[i-(m)*tau:i:tau]
    Sj=fit[-xj.shape[1]:]
    #define test vectors and correct predictions
    xi=np.zeros((m+1,test.size-P-1-(m-1)*tau))
    xi[0,:]=1
    for i in range (1+m*tau,test.size-P):
        xi[1:,i-1-m*tau]=test[i-(m)*tau:i:tau]
    Si=test[-xi.shape[1]:]
    #subsampling
    xi=xi[:,::subi]
    Si=Si[::subi]
    if subj>=1:
        xj=xj[:,::int(subj)]
        Sj=Sj[::int(subj)]
    else:
        sk=int((1-subj)**-1)
        lo=np.mod(np.arange(Sj.size),sk)!=0
        xj=xj[:,lo]
        Sj=Sj[lo]
    #predicted values
    V=[]
    #True values to return
    Sr=[]
    for i in range(xi.shape[1]):
        if skip and Si[i]<=0:
            continue
        if i in tSet:
            continue
        #calculate distances and sort for closest
        #max norm of dif vector
#            d=np.max(np.abs(xj.T-xi[:,i]),1)
#            ind=d.argsort()[:k][::-1]
        #n-norm of dif vector
        d=np.linalg.norm(xj.T-xi[:,i],ord=2,axis=1)
        ind=np.argpartition(d,k)[:k]
        bad=np.where(d==0)
        if len(d[np.where(d>0)])==0:
            d[bad]=1
        else:
            d[bad]=min(d[np.where(d>0)])/10
        #define lin regression and solve
        W=np.sqrt(np.diag(d[ind]**-p))
        b=np.dot(Sj[ind],W)
        A=np.dot(W,(xj[:,ind]).T)
        alpha=np.linalg.lstsq(A,b)[0]
        #calculate error for this test
        #np.dot(alpha,xi[:,i])
        V.append(np.dot(alpha,xi[:,i]))
        Sr.append(Si[i])
#    if skip:
#        return np.array(V),Si[np.where(Si>0)]
    return np.array(V),np.array(Sr)

def predictability_Combo(xj_av,Sj_av,fit,test,m=2,tau=1,P=1,k=1,p=1,subi=1,subj=1,relW=1):
    #Prediction algorithm--avCompression/extrap+original data
    #Define naive fitting vectors and predictions
    xj=np.zeros((m+1,fit.size-P-1-(m-1)*tau))
    xj[0,:]=1#=np.zeros
    for i in range (1+m*tau,fit.size-P):
        xj[1:,i-1-m*tau]=fit[i-(m)*tau:i:tau]
    Sj=fit[-xj.shape[1]:]
#    #Append avalanche extrapolation
#    xj=np.append(xj,xj_av,1)
#    Sj=np.append(Sj,Sj_av) 
#    xj[0,:]=0
    
    #define test vectors and correct predictions
    xi=np.zeros((m+1,test.size-P-1-(m-1)*tau))
    xi[0,:]=1
    for i in range (1+m*tau,test.size-P):
        xi[1:,i-1-m*tau]=test[i-(m)*tau:i:tau]
    Si=test[-xi.shape[1]:]
    #subsampling
    xi=xi[:,::subi]
    Si=Si[::subi]
    xj=xj[:,::subj]
    Sj=Sj[::subj]
    #predicted values
    V=[]
    for i in range(xi.shape[1]):
        #calculate distances and sort for closest
        #max norm of dif vector
#        d=np.max(np.abs(xj.T-xi[:,i]),1)
#        ind=d.argsort()[:k][::-1]
        #n-norm of dif vector
        d=np.linalg.norm(xj.T-xi[:,i],ord=2,axis=1)
#        ind=d.argsort()[:k][::-1]
        ind=np.argpartition(d,k)[:k]
        bad=np.where(d==0)
        if len(d[np.where(d>0)])==0:
            d[bad]=1
        else:
            d[bad]=min(d[np.where(d>0)])/10
        d_av=np.linalg.norm(xj_av.T-xi[:,i],ord=2,axis=1)
#        ind=d.argsort()[:k][::-1]
        ind_av=np.argpartition(d_av,k)[:k]
        bad_av=np.where(d_av==0)
        if len(d_av[np.where(d_av>0)])==0:
            d_av[bad_av]=1
        else:
            d_av[bad_av]=min(d_av[np.where(d_av>0)])/10
        #define lin regression and solve
        vec=np.append(xj[:,ind],xj_av[:,ind_av],1)
        sol=np.append(Sj[ind],Sj_av[ind_av])
        dist=np.append(relW*d[ind],d_av[ind_av])
        
        W=np.sqrt(np.diag(dist**-p))
        b=np.dot(sol,W)
        A=np.dot(W,(vec).T)
        alpha=np.linalg.lstsq(A,b)[0]
        #calculate error for this test
        #np.dot(alpha,xi[:,i])
        V.append(max(np.dot(alpha,xi[:,i]),0))
    return np.array(V),Si



def avalancheLoc(data,thresh):
    st=[]
    en=[]
    for i in range (data.size):
        if len(st)==len(en):
            if data[i]>thresh:
                st.append(i)
            continue
        if len(st)>len(en):
            if data[i]<thresh:
                en.append(i)
    if len(st)>len(en):
        en.append(data.size)
    st=np.array(st)
    en=np.array(en)
    #correct for low data cases
    if len(st)==0:
        st=[0]
        en=[data.size-1]    
    return st,en   

#define diagnostic metrics
def diagMet(test,pred):
    n=test.size
    TP=np.sum(test*pred)/n
    TN=np.sum((test-1)*(pred-1))/n
    FN=-1*np.sum((test)*(pred-1))/n
    FP=-1*np.sum((test-1)*(pred))/n
    res=[[TP,FP],[FN,TN]]
    res=np.array(res)
    acc=(TP+TN)/(np.sum(res))
    sen=TP/(TP+FN)
    sp=TN/(TN+FP)
    return acc,sen,sp,res  

#Useful func
def binAvg(x,y,bi=np.linspace(0,1,10)):
    n1,b,p=plt.hist(x,bins=bi)
    n2,b,p=plt.hist(x,bins=bi,weights=y)
    plt.close('all')
    return n2/n1

# In[]
###MANY SPECIES
#mus Data   
xmus=np.load('mussels/species.npy')
#herbPlankton
xplank=np.load('plankton/species.npy')
#fish
xfish=np.load('fish/species.npy')[:,2:-3]

def removeLongZeros(x,length):
    #Function to remove zero abundance stretches from data
    i=0
    while i<x.size-length:
        if np.sum(x[i:i+length])==0:
            x=np.delete(x,np.arange(i,i+length))
            print(i)
        i+=1
            
    return x


# In[]
#PERFORMANCE FOR FIGURE 2
def performance(DATA,Nfit,m,tau,P,k,p,shf=1):
    subi=1
    subj=1
    mae_a=[]
    mae_n=[]
    rel_a=[]
    rel_n=[]
    TRUTH=[]
    AV=[]
    NI=[]
    FIT=[]
    #Define GENERIC family of curves
    Tset=np.append(np.linspace(2,100,98),np.linspace(100,1000,400)).astype(int)#np.linspace(2,200,200).astype(int)
    t_boot,fit_boot=avFamily('general/normShape.npy',Tset,1)
    xg,sg=fitSet(fit_boot,m,tau,P,sub=1,zeroBuf=0)
    #Pull in Data, remove mean, define test set
    dat=DATA.copy()
#    dat=dat-shf*np.mean(dat)
    test=dat[:]
    #test for every fit set
    for i in range(1,DATA.size-(m*tau+P+k)):
        #sub sample training sets
        if i%1>0:
            continue
        #Define fit set
        fit=dat[i:i+Nfit].copy()
#        #skip if constant valued set--include these for percentile performance comp
#        if len(np.where(fit==np.mean(fit)))==len(fit):
#            continue
        #Calculate the system specific scaling variable b
        t,s=avalancheDist_mult(fit-np.mean(fit),0)
        if len(t)<1:
            print(i)
        if len(t)==0: #Handles exception when no variation in fit data
            b=np.var(fit)
        else:
            b=bFit(t,s)
        #make SPECIFIC family of avalanche curves
        xj=xg.copy()*b
        sj=sg.copy()*b
        #Run prediction
        buf=0 #buffer values around training data not predicted
        exc=np.arange(max(0,i-buf),min(i+Nfit+buf,test.size))
        pred,truth=predictability_Av(xj,sj,test,m,tau,P,k,p,subi,subj,True,tSet=exc)
        pred_n,truth_n=predictability_naive(fit,test,m,tau,P,k,p,subi,subj,True,tSet=exc)
        #Remove indices of fitting set--now Handled in predictability function
#        exc=np.arange(i,i+Nfit)
#        pred=np.delete(pred,exc)
#        pred_n=np.delete(pred_n,exc)
#        truth=np.delete(truth,exc)
#        truth_n=np.delete(truth_n,exc)
        #Check MAE of each method
        ind=np.where(truth>0)
        mae_a.append(np.mean(np.abs(pred[ind]-truth[ind]))/np.mean(truth[ind]))
        mae_n.append(np.mean(np.abs(pred_n[ind]-truth[ind]))/np.mean(truth[ind]))
        #Check RelError of each method
        ind=np.where(truth>0)
        rel_a.extend(np.abs(pred[ind]-truth[ind])/truth[ind])
        rel_n.extend(np.abs(pred_n[ind]-truth[ind])/truth[ind])
        #Give back true values and predictions
        TRUTH.append(truth)
        AV.append(pred)
        NI.append(pred_n)
        FIT.append(fit.copy())
#    return ([np.mean(mae_a),np.std(mae_a)],[np.mean(mae_n),np.std(mae_n)])
        del xj
        del sj
    return TRUTH,AV,NI,mae_a,mae_n,rel_a,rel_n,FIT


m=3
tau=1
P=1
k=2
p=1
const=(m,tau,P,k,p)
endt=-1


NFIT=10
A=[]
N=[]
rA=[]
rN=[]
T=[]
PA=[]
PN=[]
ID=[]
FIT=[]

#algea
at=time.time()
D=np.load('mussels/algae_inter.npy')
t,pa,pn,a,n,ra,rn,f=performance(D,NFIT,*const)
A.append(a)
N.append(n)
rA.append(np.array(ra))
rN.append(np.array(rn))
ID.append('algea')
T.append(np.array(t))
PA.append(np.array(pa))
PN.append(np.array(pn))
FIT.append(f)
print(time.time()-at)

#mussel
at=time.time()
D=np.load('mussels/mussel_inter.npy')
#D=D-np.mean(D)
t,pa,pn,a,n,ra,rn,f=performance(D,NFIT,*const)
A.append(a)
N.append(n)
rA.append(np.array(ra))
rN.append(np.array(rn))
ID.append('mussel')
T.append(np.array(t))
PA.append(np.array(pa))
PN.append(np.array(pn))
FIT.append(f)
print(time.time()-at)

#barnacle
at=time.time()
D=np.load('mussels/barnacle_inter.npy')
#D=D-np.mean(D)
t,pa,pn,a,n,ra,rn,f=performance(D,NFIT,*const)
A.append(a)
N.append(n)
rA.append(np.array(ra))
rN.append(np.array(rn))
ID.append('barnacle')
T.append(np.array(t))
PA.append(np.array(pa))
PN.append(np.array(pn))
FIT.append(f)
print(time.time()-at)
"""
#Rockfish
at=time.time()
D=xfish[:endt,3]
D=removeLongZeros(D,NFIT)
t,pa,pn,a,n,ra,rn,f=performance(D,NFIT,*const)
A.append(a)
N.append(n)
rA.append(np.array(ra))
rN.append(np.array(rn))
ID.append('rockfish')
T.append(np.array(t))
PA.append(np.array(pa))
PN.append(np.array(pn))
FIT.append(f)
print(time.time()-at)

#mackeral
at=time.time()
D=xfish[:endt,4]
D=removeLongZeros(D,NFIT)
#D=D-np.mean(D)
t,pa,pn,a,n,ra,rn,f=performance(D,NFIT,*const)
A.append(a)
N.append(n)
rA.append(np.array(ra))
rN.append(np.array(rn))
ID.append('mackeral')
T.append(np.array(t))
PA.append(np.array(pa))
PN.append(np.array(pn))
FIT.append(f)
print(time.time()-at)

#goby
at=time.time()
D=xfish[:endt,-4]
D=removeLongZeros(D,NFIT)
#D=D-np.mean(D)
t,pa,pn,a,n,ra,rn,f=performance(D,NFIT,*const)
A.append(a)
N.append(n)
rA.append(np.array(ra))
rN.append(np.array(rn))
ID.append('goby')
T.append(np.array(t))
PA.append(np.array(pa))
PN.append(np.array(pn))
FIT.append(f)
print(time.time()-at)

#calanoid
at=time.time()
D=np.load('plankton/calanoid_inter.npy')
D=D-np.min(D)
#D=D-np.mean(D)
t,pa,pn,a,n,ra,rn,f=performance(D,NFIT,*const,shf=.1)
A.append(a)
N.append(n)
rA.append(np.array(ra))
rN.append(np.array(rn))
ID.append('calanoid')
T.append(np.array(t))
PA.append(np.array(pa))
PN.append(np.array(pn))
FIT.append(f)
print(time.time()-at)

#Rotifer
at=time.time()
D=np.load('plankton/rotifer_inter.npy')
D=D-np.min(D)
#D=D-np.mean(D)
t,pa,pn,a,n,ra,rn,f=performance(D,NFIT,*const,shf=.1)
A.append(a)
N.append(n)
rA.append(np.array(ra))
rN.append(np.array(rn))
ID.append('rotifer')
T.append(np.array(t))
PA.append(np.array(pa))
PN.append(np.array(pn))
FIT.append(f)
print(time.time()-at)

#Protazoa
at=time.time()
D=np.load('plankton/protozoa_inter.npy')
D=D-np.min(D)
#D=D-np.mean(D)
t,pa,pn,a,n,ra,rn,f=performance(D,NFIT,*const,shf=.1)
A.append(a)
N.append(n)
rA.append(np.array(ra))
rN.append(np.array(rn))
ID.append('protazoa')
T.append(np.array(t))
PA.append(np.array(pa))
PN.append(np.array(pn))
FIT.append(f)
print(time.time()-at)
"""
# In[]
"""
#Save results
#Results=(A,N,rA,rN,ID,T,PA,PN,FIT)
#with open('envelopeResults_natData_interpolate_allFit50.pickle', 'wb') as f:
#        pickle.dump(Results, f)

##Load
with open('envelopeResults_natData_interpolate_allFit.pickle', "rb") as input_file:
    e = pickle.load(input_file)
(A,N,rA,rN,ID,T,PA,PN,FIT)=e
#
#def unpk(A,N,rA,rN,ID,T,PA,PN):
#    return A,N,rA,rN,ID,T,PA,PN
#
#A,N,rA,rN,ID,T,PA,PN=unpk(*e)
"""
Results=(A,N,rA,rN,ID,T,PA,PN,FIT)
with open('envelopeResults_091619_gam15.pickle', 'wb') as f:
        pickle.dump(Results, f)


# In[]
#FIT ORDER HIGH PREDICTIONS
fig,ax=plt.subplots(nrows=3,ncols=3,figsize=(10,10),sharex=True,sharey=True)
fig2,ax2=plt.subplots(nrows=3,ncols=3,figsize=(10,10),sharex=True)

Xs=[0,0,0,1,1,1,2,2,2]
Ys=[0,1,2,0,1,2,0,1,2]
for h in range(len(T)):
    #define species of interest
    #h=-1
    #Fit by percentile
    perc=[0,10,20,30,40,50,60,70,80,90,100]
    perc=np.linspace(0,100,11)
    err_a=[]
    err_a_hi=[]
    err_a_lo=[]
    err_n=[]
    err_n_hi=[]
    err_n_lo=[]
    test_perc=95
    clr=['orange','purple']
    #define metric for fits
    fit_met=[]
    for f in FIT[h]:
        fit_met.append(np.mean(f))
#        fit_met.append((np.max(f)+.01)/(np.min(f)+.01))
#        if np.sum(f)>0:
#            fit_met.append(entropy(f))
#        else:
#            fit_met.append(0)
       


#    ### INNER QUARTILE 
#    for i in range(1,len(perc)):
#        #find fit sets in percentile range
#        ind=np.where((fit_met<=np.percentile(fit_met,perc[i]))&(fit_met>=np.percentile(fit_met,perc[i-1])))[0]
##        ind=np.where((fit_met<=np.percentile(fit_met,perc[i])))[0]
#        #append appropriate errors
#        e_a=[]
#        e_n=[]
#        for j in ind:
#            ind2=np.where(T[h][j]>np.percentile(T[h][j],test_perc))[0]
#            e_a.extend(np.abs(T[h][j][ind2]-PA[h][j][ind2])/np.mean(T[h][j][ind2]))
#            e_n.extend(np.abs(T[h][j][ind2]-PN[h][j][ind2])/np.mean(T[h][j][ind2]))
#        #append results for this percentile
#        err_a.append(np.median(e_a))
#        err_n.append(np.median(e_n))
#        ph=75
#        pl=25
#        err_a_hi.append(np.percentile(e_a,ph)-np.median(e_a))
#        err_a_lo.append(-np.percentile(e_a,pl)+np.median(e_a))
#        err_n_hi.append(np.percentile(e_n,ph)-np.median(e_n))
#        err_n_lo.append(-np.percentile(e_n,pl)+np.median(e_n))
  
        
        ### MAE
    for i in range(1,len(perc)):
        #find fit sets in percentile range
        ind=np.where((fit_met<=np.percentile(fit_met,perc[i]))&(fit_met>=np.percentile(fit_met,perc[i-1])))[0]
#        ind=np.where((fit_met<=np.percentile(fit_met,perc[i])))[0]
        #append appropriate errors
        e_a=[]
        e_n=[]
        for j in ind:
            ind2=np.where(T[h][j]>np.percentile(T[h][j],test_perc))[0]
            e_a.append(np.mean(np.abs(T[h][j][ind2]-PA[h][j][ind2])/np.mean(T[h][j][ind2])))
            e_n.append(np.mean(np.abs(T[h][j][ind2]-PN[h][j][ind2])/np.mean(T[h][j][ind2])))
        #append results for this percentile
        err_a.append(np.median(e_a))
        err_n.append(np.median(e_n))
        ph=75
        pl=25
        err_a_hi.append(np.percentile(e_a,ph)-np.median(e_a))
        err_a_lo.append(-np.percentile(e_a,pl)+np.median(e_a))
        err_n_hi.append(np.percentile(e_n,ph)-np.median(e_n))
        err_n_lo.append(-np.percentile(e_n,pl)+np.median(e_n))
        
        
    #Plot Results from this species
    xp=Xs[h]
    yp=Ys[h]
    ax[xp,yp].scatter(perc[1:],err_a,c=clr[0],zorder=2)    
    ax[xp,yp].scatter(perc[1:],err_n,c=clr[1],zorder=2)    
    ax[xp,yp].errorbar(perc[1:],err_a,yerr=(err_a_lo,err_a_hi),c=clr[0],capsize=5,fmt='none',zorder=-2,alpha=.5)    
    ax[xp,yp].errorbar(perc[1:],err_n,yerr=(err_n_lo,err_n_hi),c=clr[1],capsize=5,fmt='none',zorder=-2,alpha=.5)    
    ax[xp,yp].set_title(ID[h])
    #plot percentile curve in fig2
    qp=np.linspace(0,100,100)
    ax2[xp,yp].plot(qp,np.percentile(fit_met,qp))
    qt=np.percentile(T[h][j],test_perc)
    ax2[xp,yp].plot([0,100],[qt,qt])
    
#plt.tight_layout()
#fig.subplots_adjust(wspace=None, hspace=None)
fig.text(0.5, 0.04, 'Fit Percentile', ha='center')
#fig.text(0.04, 0.5, 'Normalized AE', va='center', rotation='vertical')
fig.text(0.04, 0.5, 'Normalized MAE', va='center', rotation='vertical')
ax[0,0].set_xlim(5,105)
ax[0,0].set_ylim(0,1.1)


#fig.savefig('natPredict_rankedPercentile_MAE95.svg')

















# In[]
#Violin plot AE


x=[1,2,4,5,7,8,10,11,13,14,16,17,19,20,22,23]
h=[]
clr=['orange','purple']
cList=[]
mx=10
for i in range(len(A)):
#    y=np.abs(PA[i]-T[i])/np.mean(T[i])
#    ind=np.where(np.array(y)<1000)
#    h.append(y[ind])
#    y=np.abs(PN[i]-T[i])/np.mean(T[i])
#    ind=np.where(np.array(y)<1000)
#    h.append(y[ind])
#    cList.append(clr[0])
#    cList.append(clr[1])
    th=np.percentile(T[i],80)
    y=((PA[i]-T[i])/np.mean(T[i]))
    ind=np.where(np.array(T[i])>th)
    y=y[ind]
    ind=np.where(np.array(y)<mx)
    y=y[ind]
    ind=np.where(np.array(y)>-mx)
    h.append(y[ind])
    
    y=((PN[i]-T[i])/np.mean(T[i]))
    ind=np.where(np.array(T[i])>th)
    y=y[ind]
    ind=np.where(np.array(y)<mx)
    y=y[ind]
    ind=np.where(np.array(y)>-mx)
    h.append(y[ind])
    cList.append(clr[0])
    cList.append(clr[1])

fig=plt.figure()
violin_parts=plt.violinplot(h,positions=x,showextrema=False,showmedians=False,points=100)

for i,pc in enumerate(violin_parts['bodies']):
    pc.set_alpha(1)
    if i%2==0:
        pc.set_facecolor(clr[0])
    else:
        pc.set_facecolor(clr[1])
#    pc.set_edgecolor('black')
plt.ylim(-7.5,7.5)
plt.xticks([1.5,4.5,7.5,10.5,13.5,16.5,19.5,22.5])
plt.xlim(0,24)
plt.plot([0,24],[0,0],c='grey',zorder=-2,alpha=.5)

#fig.savefig('natPredict_violin_v1.svg')

# In[]

fig=plt.figure()
x=[1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8]
violin_parts=plt.violinplot(h,positions=x,showextrema=False,showmedians=False,points=1001)
mn_a=[]
mn_n=[]
for i,q in enumerate(h):
    if i%2==0:
        mn_a.append(np.mean(np.abs(q)))
    else:
        mn_n.append(np.mean(np.abs(q)))

for i,pc in enumerate(violin_parts['bodies']):
    if i%2==0:
        pc.set_facecolor(clr[0])
        pc.set_alpha(.8)
    else:
        pc.set_facecolor(clr[1])
        pc.set_zorder(-2)
        pc.set_alpha(.8)
#    pc.set_edgecolor('black')
plt.ylim(-7.5,7.5)
plt.xticks([1,2,3,4,5,6,7,8])
plt.xlim(0,9)
plt.plot([0,24],[0,0],c='grey',zorder=-2,alpha=.5)

#plt.plot([0,24],[1,1],c='grey',zorder=-2,alpha=.5)
#plt.scatter([1,2,3,4,5,6,7,8],mn_a,c=clr[0])
#plt.scatter([1,2,3,4,5,6,7,8],mn_n,c=clr[1])
#fig.savefig('natPredict_violin_v2.svg')




# In[]
i=0
from scipy.stats import pearsonr
from sklearn.metrics import r2_score
ca=pearsonr(T[i],PA[i])[0]
cn=pearsonr(T[i],PN[i])[0]
ra=r2_score(T[i],PA[i])
rn=r2_score(T[i],PN[i])
#
#ind=np.where(T[i]<np.mean(T[i]))
#ra_l=r2_score(T[i][ind],PA[i][ind])
#rn_l=r2_score(T[i][ind],PN[i][ind])
#ind=np.where(T[i]>np.mean(T[i]))
#ra_h=r2_score(T[i][ind],PA[i][ind])
#rn_h=r2_score(T[i][ind],PN[i][ind])
#
#
#en=max(T[i])
#mn=np.mean(T[i])
##bi=np.linspace(0,en+1,20)
##pab=binAvg(T[i],PA[i],bi)
##pnb=binAvg(T[i],PN[i],bi)
##al=1
##plt.scatter(bi[:-1],pab,c=clr[0],alpha=al)
##plt.scatter(bi[:-1],pnb,c=clr[1],alpha=al)
#
#al=.1
#plt.scatter(T[i],PA[i],c=clr[0],alpha=al,label=str('Av, C='+str(ca)+', R2='+str(ra)))
#plt.scatter(T[i],PN[i],c=clr[1],alpha=al,zorder=-1,label=str('Av, C='+str(cn)+', R2='+str(rn)))
#
#plt.plot([0,en],[0,en])
#plt.plot([mn,mn],[0,mn],c='grey')
#plt.plot([0,mn],[mn,mn],c='grey')
#plt.ylim(-en,3*en)
#plt.legend()
#
#print(ra_l,rn_l,ra_h,rn_h)

#DO ALL
fig,ax=plt.subplots(nrows=3,ncols=3,figsize=(12,10))
al=.05
Xs=[0,0,1,1,1,2,2,2]
Ys=[0,1,0,1,2,0,1,2]
for i in range(len(T)):
    ca=pearsonr(T[i],PA[i])[0]
    cn=pearsonr(T[i],PN[i])[0]
    ra=r2_score(T[i],PA[i])
    rn=r2_score(T[i],PN[i])
    en=max(T[i])
#    xp=int(i%3)
#    yp=int(np.trunc(i/3.))
    xp=Xs[i]
    yp=Ys[i]
    ax[xp,yp].scatter(T[i],PA[i],c=clr[0],alpha=al,label=str('Av, C='+str(ca)+', R2='+str(ra)),rasterized=True)
    ax[xp,yp].scatter(T[i],PN[i],c=clr[1],alpha=al,zorder=-1,label=str('Av, C='+str(cn)+', R2='+str(rn)),rasterized=True)
    ax[xp,yp].plot([0,en],[0,en])
    ax[xp,yp].set_ylim(-en,2*en)
    ax[xp,yp].legend(title=ID[i])
# In[]
#HIST CHECK
i=4
fip=plt.figure()
bi=np.linspace(-10,10,1000)
al=.4
w=np.ones(len(h[2*i]))/len(h[2*i])
plt.hist(h[2*i],bins=bi,color=clr[0],alpha=al,weights=w)
w=np.ones(len(h[2*i+1]))/len(h[2*i+1])
plt.hist(h[2*i+1],bins=bi,color=clr[1],alpha=al,weights=w)






# In[]
#Check Results
i=-1
al=.6
plt.plot(T[i],alpha=al)
plt.plot(PA[i],alpha=al)
plt.plot(PN[i],alpha=al)
#plt.plot(PA[i]-T[i],alpha=al)
#plt.plot(PN[i]-T[i],alpha=al)
plt.ylim(0,10)
# In[]
# Bar plot of results-MAE
x=[1,2,4,5,7,8,10,11,13,14,16,17,19,20,22,23]

h=[]
er=[]
er_L=[]
er_H=[]
pl=25
ph=75
clr=['orange','purple']
cList=[]
for i in range(len(A)):
    h.append(np.median(A[i]))
#    h.append(np.median(A[i]))
    er.append(np.std(A[i]))
    er_L.append(np.median(A[i])-np.percentile(A[i],pl))
    er_H.append(np.percentile(A[i],ph)-np.median(A[i]))
    ind=np.where(np.array(N[i])<10)
    N[i]=np.array(N[i])
    h.append(np.median(N[i][ind]))
#    h.append(np.median(N[i][ind]))
    er.append(np.std(N[i][ind]))
    er_L.append(np.median(N[i][ind])-np.percentile(N[i][ind],pl))
    er_H.append(np.percentile(N[i][ind],ph)-np.median(N[i][ind]))
    cList.append(clr[0])
    cList.append(clr[1])

fig=plt.figure()

en=len(x)
#plt.bar(x[:en],h[:en],yerr=er[:en],color=cList[:en],ecolor='grey',capsize=5)
plt.bar(x[:en],h[:en],yerr=(er_L[:en],er_H[:en]),color=cList[:en],ecolor='grey',capsize=3)

yr=[1,1]
al=.5
xr=[0,6]
plt.fill_between(xr,yr,zorder=-2,color='paleturquoise',alpha=al)
xr=[6,15]
plt.fill_between(xr,yr,zorder=-2,color='palegreen',alpha=al)
xr=[15,24]
plt.fill_between(xr,yr,zorder=-2,color='khaki',alpha=al)

plt.ylabel('Relative MAE')
plt.xticks([1.5,4.5,7.5,10.5,13.5,16.5,19.5,22.5])
plt.xlim(0,24)
#plt.ylim(0,1)
plt.title(ID)
#fig.savefig('smartPredict_performance.svg')

# In[]
# Bar plot of results-Rel
x=[1,2,4,5,7,8,10,11,13,14,16,17,19,20,22,23]

h=[]
er=[]
er_L=[]
er_H=[]
pl=25
ph=75
clr=['orange','purple']
cList=[]
for i in range(len(rA)):
#    h.append(np.mean(rA[i]))
    h.append(np.median(rA[i]))
    er.append(np.std(rA[i]))
    er_L.append(np.percentile(rA[i],pl))
    er_H.append(np.percentile(rA[i],ph))
    ind=np.where(np.array(rN[i])>-10**6)
#    N[i]=np.array(rN[i])
#    h.append(np.mean(rN[i][ind]))
    h.append(np.median(rN[i][ind]))
    er.append(np.std(rN[i][ind]))
    er_L.append(np.percentile(rN[i][ind],pl))
    er_H.append(np.percentile(rN[i][ind],ph))
    cList.append(clr[0])
    cList.append(clr[1])

fig=plt.figure()

en=len(x)
#plt.bar(x[:en],h[:en],yerr=er[:en],color=cList[:en],ecolor='grey',capsize=5)
plt.bar(x[:en],h[:en],yerr=(er_L[:en],er_H[:en]),color=cList[:en],ecolor='grey',capsize=3)

yr=[5,5]
al=.2
xr=[0,6]
plt.fill_between(xr,yr,zorder=-2,color='paleturquoise',alpha=al)
xr=[6,15]
plt.fill_between(xr,yr,zorder=-2,color='palegreen',alpha=al)
xr=[15,24]
plt.fill_between(xr,yr,zorder=-2,color='khaki',alpha=al)

plt.ylabel('Percent Error')
plt.xticks([1.5,4.5,7.5,10.5,13.5,16.5,19.5,22.5])
plt.xlim(0,24)
plt.ylim(0,5)
plt.title(ID)
#fig.savefig('smartPredict_performance_rel.svg')



# In[]
#log Scale Error
x=[1,2,4,5,7,8,10,11,13,14,16,17,19,20,22,23]
n=8
rel_a=[[] for x in range(n)]
rel_n=[[] for x in range(n)]

for i in range(len(rel_a)):
    ind=np.where(T[i]>0)
    At=(PA[i][ind])
    Nt=(PN[i][ind])
    Tr=(T[i][ind])
    At=np.array(At)
    Nt=np.array(Nt)
    Tr=np.array(Tr)
    Nt[np.where(~np.isfinite(Nt))]=min(Tr)
    At[np.where(~np.isfinite(At))]=min(Tr)
    rel_a[i]=np.abs(At-Tr)/np.mean(Tr)
    rel_n[i]=np.abs(Nt-Tr)/np.mean(Tr)
    print(min(Tr),np.mean(Tr))

h=[]
er=[]
er_L=[]
er_H=[]
pl=25
ph=75
clr=['orange','purple']
cList=[]
for i in range(len(rel_a)):
#    h.append(np.mean(rel_a[i]))
    h.append(np.median(rel_a[i]))
    er.append(np.std(rel_a[i]))
    er_L.append(np.percentile(rel_a[i],pl))
    er_H.append(np.percentile(rel_a[i],ph))
#    h.append(np.mean(rel_n[i]))
    h.append(np.median(rel_n[i]))
    er.append(np.std(rel_n[i]))
    er_L.append(np.percentile(rel_n[i],pl))
    er_H.append(np.percentile(rel_n[i],ph))
    cList.append(clr[0])
    cList.append(clr[1])



fig=plt.figure()

en=len(x)
#plt.bar(x[:en],h[:en],yerr=er[:en],color=cList[:en],ecolor='grey',capsize=5)
plt.bar(x[:en],h[:en],yerr=(er_L[:en],er_H[:en]),color=cList[:en],ecolor='grey',capsize=3)

yr=[5,5]
al=.2
xr=[0,6]
plt.fill_between(xr,yr,zorder=-2,color='paleturquoise',alpha=al)
xr=[6,15]
plt.fill_between(xr,yr,zorder=-2,color='khaki',alpha=al)
xr=[15,24]
plt.fill_between(xr,yr,zorder=-2,color='palegreen',alpha=al)

#plt.ylabel('Percent Error')
plt.ylabel('Normalized MAE')
plt.xticks([1.5,4.5,7.5,10.5,13.5,16.5,19.5,22.5])
plt.xlim(0,24)
plt.ylim(0,4)
plt.title(ID)


#fig.savefig('smartPredict_performance_MAEall.svg')



# In[]
#Significance test

pVal=[]
for i in range(len(rel_a)):
    pVal.append(scipy.stats.ttest_ind(rel_a[i],rel_n[i])[1] )

print(pVal)














# In[]
i=-1
bi=np.linspace(0,10,100)

plt.hist(rN[i],bins=bi)
plt.hist(rA[i],bins=bi)

# In[]
i=-5
plt.plot(T[i])
plt.plot(PA[i])
plt.plot(PN[i])
plt.ylim(-100,10000)


# In[]
#D=np.load('mussels/mussel_inter.npy')
D=np.load('plankton/calanoid_inter.npy')
D=D-np.min(D)

tau=2
Crange=plt.cm.cool
#plt.plot(D[:-tau],D[tau:])
#
#for i in range(D.size-tau-1):
#    x=[D[i],D[i+1]]
#    y=[D[i+tau],D[i+tau+1]]
#    plt.plot(x,y,c=Crange(i/D.size))


plt.scatter(D[:-tau],D[tau:],alpha=1,s=5,color=Crange(np.linspace(0,1,len(D))))
#plt.xscale('log')
#plt.yscale('log')

plt.xlabel('$x_t$')
plt.ylabel('$x_{t+\\tau}$')

# In[]
#RANK ABUNDANCE
SP=xfish#[:,-1]
Q=np.reshape(SP,SP.size)
#bi=np.logspace(0,max(Q),5)
bi=np.linspace(0,max(Q),10)
n,b,p=plt.hist(Q,bins=bi)
plt.close('all')
bx=bi[1:]-bi[:-1]
n=n/bx
n=n/np.sum(n)
plt.scatter(np.log10(bi[1:]),np.log10(n))
#plt.xscale('log')
#plt.yscale('log')

